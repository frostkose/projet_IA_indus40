{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhMlL7JeKKBk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLnMo2XtjUyE"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel(\"dataset_02052023.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8ZWa1pWxj2cV",
        "outputId": "ce322dd4-10b5-49dc-efe6-be7b322a4f19"
      },
      "outputs": [],
      "source": [
        "data.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YVxrePNUujIs",
        "outputId": "d5958b46-a83f-4952-aa0a-90f61dc26aa5"
      },
      "outputs": [],
      "source": [
        "\n",
        "data['Robot_ProtectiveStop'] = data['Robot_ProtectiveStop'].astype(bool)\n",
        "data.head(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUKb5ifpj8ab",
        "outputId": "60bb902a-32b4-4c4d-998c-98f37421a6af"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jFX8_Fmj-AE",
        "outputId": "9ad9e6df-a72a-4fd9-8141-fb6dfda606fa"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "S6yH8g1CkbY8",
        "outputId": "7043565a-82e2-4a8e-b02d-a729d0fd01d1"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "n0R0oneykgRL",
        "outputId": "754fbcf8-0082-4d45-e16a-3c01f7e0bd36"
      },
      "outputs": [],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "W2s-4BkMux0p",
        "outputId": "1b172e58-5abd-42e2-e284-b286e6be340e"
      },
      "outputs": [],
      "source": [
        " data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "kbEDJlFMvJAx",
        "outputId": "71608bf5-ff27-4cdf-ecfd-bf1c944c5d49"
      },
      "outputs": [],
      "source": [
        " data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LElYhao0b8L",
        "outputId": "b77c8fc4-a137-451a-f3ed-e1c34950660e"
      },
      "outputs": [],
      "source": [
        "data = data.dropna()\n",
        "print(data.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byEVsM7Q1OoH",
        "outputId": "1d52d1cb-bdc3-4c17-dd16-8549d81b4ae8"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "LteAJ-y8WiXi",
        "outputId": "521029ab-ab88-48e7-d15f-622287497784"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "data = pd.read_excel(\"dataset_02052023.xlsx\")\n",
        "data['Robot_ProtectiveStop'] = data['Robot_ProtectiveStop'].astype(bool)\n",
        "data = data.dropna()\n",
        "# Convert the DataFrame to CSV\n",
        "data.to_csv('cleaned_dataset.csv', index=False)\n",
        "# Download the file\n",
        "from google.colab import files\n",
        "files.download('cleaned_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40Fb_9YV1a4h",
        "outputId": "95b40069-8a51-4742-bf6d-e18991c4e147"
      },
      "outputs": [],
      "source": [
        "def remove_rows_with_many_zeros(df, threshold=0.2):\n",
        "    \"\"\"Removes rows where the percentage of zeros exceeds the given threshold.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "        threshold: The maximum percentage of zeros allowed in a row (default is 0.2 or 20%).\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with rows containing excessive zeros removed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Select numerical columns only for zero count calculation\n",
        "    numerical_cols = df.select_dtypes(include=np.number).columns\n",
        "\n",
        "    # Calculate the percentage of zeros in each row for numerical columns\n",
        "    zero_percentage = (df[numerical_cols] == 0).sum(axis=1) / len(numerical_cols)\n",
        "\n",
        "    # Filter out rows exceeding the threshold\n",
        "    filtered_df = df[zero_percentage <= threshold]\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "# Apply the function to your DataFrame\n",
        "data_cleaned = remove_rows_with_many_zeros(data)\n",
        "\n",
        "# Print info to verify the change\n",
        "data_cleaned.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8Vygaw_5UOB",
        "outputId": "8b9727f2-9fd1-46ac-ac28-b8a158d19659"
      },
      "outputs": [],
      "source": [
        "new_db_file = \"cleaned_dataset.xlsx\"\n",
        "\n",
        "try:\n",
        "    data_cleaned.to_excel(new_db_file, index=False)  # Save to Excel without the index column\n",
        "    print(f\"New database saved to {new_db_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the database: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "kawSqbcs6JHX",
        "outputId": "4830a4fe-a526-49a0-8555-0d37c527e7ea"
      },
      "outputs": [],
      "source": [
        "data_cleaned.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4au8N8I64hN",
        "outputId": "e467792d-76f1-495b-e9ae-84213c9f437c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert 'Timestamp' column to datetime, handling mixed data types\n",
        "try:\n",
        "    data_cleaned['Timestamp'] = pd.to_datetime(data_cleaned['Timestamp'], errors='coerce')\n",
        "    # Drop rows where the conversion failed (resulting in NaT values)\n",
        "    data_cleaned = data_cleaned.dropna(subset=['Timestamp'])\n",
        "    print(\"Timestamp column converted to datetime successfully.\")\n",
        "except KeyError:\n",
        "    print(\"'Timestamp' column not found in the DataFrame.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during conversion: {e}\")\n",
        "\n",
        "\n",
        "#Display the updated dataframe\n",
        "data_cleaned.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "Q6-4YJoB666O",
        "outputId": "20151c60-240c-48fa-db12-964fb2b3a0d1"
      },
      "outputs": [],
      "source": [
        "data_cleaned.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtuVY82SIlHs",
        "outputId": "099deaed-cdc0-422d-fb69-435c4a9c5f6a"
      },
      "outputs": [],
      "source": [
        "# Sort the DataFrame by the 'Timestamp' column in ascending order (oldest to newest)\n",
        "data_sorted = data_cleaned.sort_values(by='Timestamp')\n",
        "\n",
        "# Display the first 20 rows of the sorted DataFrame\n",
        "print(data_sorted.head(20))\n",
        "\n",
        "\n",
        "sorted_db_file = \"sorted_dataset.xlsx\"\n",
        "\n",
        "try:\n",
        "    data_sorted.to_excel(sorted_db_file, index=False)  # Save to Excel without the index column\n",
        "    print(f\"Sorted database saved to {sorted_db_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the database: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoYedf2bIqcn",
        "outputId": "62034181-2cff-420d-b224-16be52a7309f"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Drop the 'Timestamp' column\n",
        "    data_sorted = data_sorted.drop('Timestamp', axis=1)\n",
        "    print(\"'Timestamp' column dropped successfully.\")\n",
        "\n",
        "    # Save the modified DataFrame to a new Excel file\n",
        "    final_db_file = \"final_dataset.xlsx\"\n",
        "    data_sorted.to_excel(final_db_file, index=False)\n",
        "    print(f\"Final database without 'Timestamp' saved to {final_db_file}\")\n",
        "\n",
        "except KeyError:\n",
        "    print(\"'Timestamp' column not found in the DataFrame.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iDVnZtNItd3",
        "outputId": "ff434ff7-3e89-4745-8240-a97c99c420eb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    data = pd.read_excel(\"final_dataset.xlsx\")\n",
        "    # Delete the first column (assuming it's named 'Num')\n",
        "    if 'Num' in data.columns:\n",
        "        data = data.drop('Num', axis=1)\n",
        "        print(\"'Num' column dropped successfully.\")\n",
        "\n",
        "        # Save the modified DataFrame to a new Excel file\n",
        "        final_db_file = \"final_dataset_no_num.xlsx\"\n",
        "        data.to_excel(final_db_file, index=False)\n",
        "        print(f\"Final database without 'Num' column saved to {final_db_file}\")\n",
        "\n",
        "    else:\n",
        "        print(\"'Num' column not found in the DataFrame.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"'final_dataset.xlsx' not found. Please make sure the file exists in the current directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VuOqTzBpdT59",
        "outputId": "e51d16ec-e6be-496e-e6fd-4a40d7da4742"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "try:\n",
        "    data = pd.read_excel(\"final_dataset_no_num.xlsx\")\n",
        "\n",
        "    # Calculate the correlation matrix\n",
        "    correlation_matrix = data.corr()\n",
        "\n",
        "    # Display the correlation matrix\n",
        "    print(correlation_matrix)\n",
        "\n",
        "    # Visualize the correlation matrix using a heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title(\"Correlation Matrix of Variables\")\n",
        "    plt.show()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"'final_dataset_no_num.xlsx' not found. Please make sure the file exists in the current directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGtEi-zTJPy2"
      },
      "outputs": [],
      "source": [
        "Y = data[['Robot_ProtectiveStop','grip_lost']].values\n",
        "X = data.drop(columns=['Robot_ProtectiveStop','grip_lost']).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "0uAj_kt3jBzJ",
        "outputId": "a85dd350-2bed-43b2-b795-103a71ed5ef9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "try:\n",
        "    data = pd.read_excel(\"final_dataset_no_num.xlsx\")\n",
        "\n",
        "    # Select numerical features for standardization\n",
        "    numerical_features = data.select_dtypes(include=np.number).columns\n",
        "\n",
        "    # Create a StandardScaler object\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit and transform the numerical features\n",
        "    data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
        "\n",
        "    # Save the standardized data to a new Excel file\n",
        "    standardized_file = \"standardized_final_dataset.xlsx\"\n",
        "    data.to_excel(standardized_file, index=False)\n",
        "    print(f\"Standardized data saved to {standardized_file}\")\n",
        "\n",
        "    # Download the file\n",
        "    files.download(standardized_file)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"'final_dataset_no_num.xlsx' not found. Please make sure the file exists in the current directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHVhjFz8jWxG",
        "outputId": "674f223e-ef1d-4c39-8bfe-f95e96f0f914"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "try:\n",
        "    # Load the standardized dataset\n",
        "    data = pd.read_excel(\"/content/standardized_final_dataset.xlsx\")\n",
        "\n",
        "    # Separate majority and minority classes\n",
        "    majority_class = data[data['Robot_ProtectiveStop'] == False]\n",
        "    minority_class = data[data['Robot_ProtectiveStop'] == True]\n",
        "\n",
        "    # Upsample minority class\n",
        "    minority_upsampled = resample(minority_class,\n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=len(majority_class),    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "\n",
        "    # Combine majority class with upsampled minority class\n",
        "    balanced_data = pd.concat([majority_class, minority_upsampled])\n",
        "\n",
        "    # You now have 'balanced_data' with equal class representation.\n",
        "    # Save the balanced data to a new file if needed\n",
        "    balanced_file = \"balanced_dataset.xlsx\"\n",
        "    balanced_data.to_excel(balanced_file, index=False)\n",
        "    print(f\"Balanced data saved to {balanced_file}\")\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"'standardized_final_dataset.xlsx' not found. Please make sure the file exists in the current directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI9odoYiJkR0"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from joblib import dump, load\n",
        "\n",
        "def normalisation(X):\n",
        "  scaler = MinMaxScaler()\n",
        "  normalized_X = scaler.fit_transform(X)\n",
        "  return normalized_X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikGmCASgJrP0"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "def standardisation(X):\n",
        "  scaler = StandardScaler()\n",
        "  standardized_X = scaler.fit_transform(X)\n",
        "  return standardized_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsRhwNkYJt2E"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from joblib import dump, load\n",
        "\n",
        "def normalisation(X):\n",
        "  # Reshape to 2D for MinMaxScaler (samples, features)\n",
        "  num_samples = X.shape[0]\n",
        "  num_features = X.shape[1] * X.shape[2] # Total features considering seq_length\n",
        "  X_2D = X.reshape(num_samples, num_features)\n",
        "\n",
        "  scaler = MinMaxScaler()\n",
        "  normalized_X_2D = scaler.fit_transform(X_2D)\n",
        "\n",
        "  # Reshape back to 3D\n",
        "  normalized_X = normalized_X_2D.reshape(X.shape)\n",
        "  return normalized_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcRU61OCJxZ7"
      },
      "outputs": [],
      "source": [
        "def creationSequences(X, Y, seq_length):\n",
        "    seqX, seqY = [], []\n",
        "    for i in range(0, len(data) - seq_length, 1):\n",
        "        seqX.append(X[i:(i + seq_length)])\n",
        "        seqY.append(Y[i + seq_length])\n",
        "    seqX = np.array(seqX)\n",
        "    seqY = np.array(seqY)\n",
        "    return seqX, seqY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2uY8znXJ0_b"
      },
      "outputs": [],
      "source": [
        "seq_length = 10\n",
        "X, y = creationSequences(X, Y, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ-kk246J3xk",
        "outputId": "d8e48e1f-fd68-479c-855a-c2be708e74ce"
      },
      "outputs": [],
      "source": [
        "print(\"Shape X:\", X.shape)\n",
        "print(\"Shape y:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vpdi-3pUo_v",
        "outputId": "bc814d05-0072-48b8-fbb1-9ccb186db5b0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "print(\"NaNValues in X and Y\")\n",
        "print(\"Number of NaN values in X:\", np.isnan(X).sum())\n",
        "print(\"Number of NaN values in Y:\", np.isnan(Y).sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTRBUVuVJ6O7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, Ytrain, Ytest= train_test_split(X, y, test_size=0.3, random_state=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JUGrsrJS9sI",
        "outputId": "8d0b95bd-3078-437e-c8fc-0aee82075b3a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    data = pd.read_excel(\"balanced_dataset.xlsx\")\n",
        "    # Convert target columns to numerical (True=1, False=0)\n",
        "    data['Robot_ProtectiveStop'] = data['Robot_ProtectiveStop'].astype(int)\n",
        "    data['grip_lost'] = data['grip_lost'].astype(int)\n",
        "\n",
        "    #Verify changes\n",
        "    print(data.head(25))\n",
        "\n",
        "    #Save updated data\n",
        "    final_db_file = \"final_dataset_converted_targets.xlsx\"\n",
        "    data.to_excel(final_db_file, index=False)\n",
        "    print(f\"Final database with converted target columns saved to {final_db_file}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"'final_dataset_no_num.xlsx' not found. Please make sure the file exists in the current directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szdV8G-fidyD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p0yvJEDNUPH",
        "outputId": "e407b91d-05dd-45d9-e913-a75597852517"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "seq_length=10\n",
        "\n",
        "nombre_feature = 20 # Corrected number of features\n",
        "\n",
        "# Définir Une architecture du  modèle LSTM. Les étudiants doivent tester plusieurs alternatives\n",
        "model2 = Sequential()\n",
        "model2.add(LSTM(units=50, return_sequences=True, input_shape=(seq_length, nombre_feature)))\n",
        "model2.add(LSTM(units=50))\n",
        "model2.add(Dense(units=2, activation='softmax')) # Changed to 2 units with sigmoid activation for binary classification\n",
        "\n",
        "\n",
        "# Compiler le modèle\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # ils peuvent tester plusieurs algo autres que ADAM\n",
        "# Entraîner le modèle avec GPU\n",
        "with tf.device('/GPU:0'):\n",
        "    model2.fit(Xtrain, Ytrain, epochs=10, batch_size=32, validation_data=(Xtest, Ytest))\n",
        "\n",
        "\n",
        "# Évaluer le modèle\n",
        "loss, accuracy = model2.evaluate(Xtest, Ytest)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StxoRoj5UCkE",
        "outputId": "1e4433b0-7bbc-4135-9f5b-03c4836c7c8e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from joblib import dump, load\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "\n",
        "seq_length=10\n",
        "\n",
        "nombre_feature = 20 # Corrected number of features\n",
        "\n",
        "# Définir Une architecture du  modèle LSTM. Les étudiants doivent tester plusieurs alternatives\n",
        "model2 = Sequential()\n",
        "model2.add(LSTM(units=50, return_sequences=True, input_shape=(seq_length, nombre_feature)))\n",
        "model2.add(LSTM(units=50))\n",
        "model2.add(Dense(units=2, activation='sigmoid')) # Use sigmoid for binary classification\n",
        "\n",
        "\n",
        "# Compiler le modèle\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # ils peuvent tester plusieurs algo autres que ADAM\n",
        "\n",
        "# Check for NaN values in your training data\n",
        "print(\"NaN values in Xtrain:\", np.isnan(Xtrain).any())\n",
        "print(\"NaN values in Ytrain:\", np.isnan(Ytrain).any())\n",
        "\n",
        "# If there are NaNs, replace them with 0 (or handle them appropriately)\n",
        "Xtrain = np.nan_to_num(Xtrain)\n",
        "Ytrain = np.nan_to_num(Ytrain)\n",
        "\n",
        "# Entraîner le modèle avec GPU (or CPU if you don't have a GPU)\n",
        "with tf.device('/GPU:0'): # or '/CPU:0' if no GPU\n",
        "    model2.fit(Xtrain, Ytrain, epochs=10, batch_size=32, validation_data=(Xtest, Ytest))\n",
        "\n",
        "\n",
        "# Évaluer le modèle\n",
        "loss, accuracy = model.evaluate(Xtest, Ytest)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "wn2tZruBbIlI",
        "outputId": "d7727cb3-872d-486b-961c-ae10bbd45c1c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1dtzbxubMbD",
        "outputId": "19edcab2-facf-4c7a-c708-a29d25fd09a5"
      },
      "outputs": [],
      "source": [
        "model.save('model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "NDm899CtbM0t",
        "outputId": "6462c247-f724-409a-a476-5a245b46cc6b"
      },
      "outputs": [],
      "source": [
        "plot_model(model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vw6lZmBbPXZ",
        "outputId": "0285e493-d813-4b3b-a402-bb2499473c60"
      },
      "outputs": [],
      "source": [
        "model2.save('second_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfLrpuLtrAm_",
        "outputId": "7550baf2-082d-4044-aa3e-ba75d41174da"
      },
      "outputs": [],
      "source": [
        "!pip install -U scikit-learn==1.3.2 scikeras==0.11.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RSoA1qzR4bm",
        "outputId": "06cffe50-f548-4c5a-95ba-49dedcf60d5d"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "\n",
        "# Define model3 architecture\n",
        "model3 = Sequential()\n",
        "model3.add(Bidirectional(LSTM(units=64, return_sequences=True, input_shape=(seq_length, nombre_feature))))\n",
        "model3.add(Dropout(0.2))  # Add dropout for regularization\n",
        "model3.add(Bidirectional(LSTM(units=64)))\n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(Dense(units=2, activation='sigmoid'))  # Sigmoid for binary classification\n",
        "\n",
        "# Compile model3\n",
        "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model3 with GPU (or CPU)\n",
        "with tf.device('/GPU:0'):  # Or '/CPU:0'\n",
        "    model3.fit(Xtrain, Ytrain, epochs=15, batch_size=64, validation_data=(Xtest, Ytest))  # Increased epochs and batch size\n",
        "\n",
        "\n",
        "# Evaluate model3\n",
        "loss, accuracy = model3.evaluate(Xtest, Ytest)\n",
        "print(f\"Model 3 - Test Loss: {loss}\")\n",
        "print(f\"Model 3 - Test Accuracy: {accuracy}\")\n",
        "\n",
        "plot_model(model3)\n",
        "model3.save('model3.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c9T1X4YFa2uQ",
        "outputId": "2dd46c85-70c6-4065-b30b-797b3b3db969"
      },
      "outputs": [],
      "source": [
        "plot_model(model3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFy0mIwGbCYk",
        "outputId": "22816137-9a07-4e76-9abc-db216d08e9dd"
      },
      "outputs": [],
      "source": [
        "model3.save('third_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbVNpeBHSSvQ",
        "outputId": "09e34128-8989-4045-dd8e-3c5fdcf81d5c"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "\n",
        "# Define model4 architecture using 1D CNN\n",
        "model4 = Sequential()\n",
        "model4.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(seq_length, nombre_feature)))\n",
        "model4.add(MaxPooling1D(pool_size=2))\n",
        "model4.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "model4.add(MaxPooling1D(pool_size=2))\n",
        "model4.add(Flatten())\n",
        "model4.add(Dense(100, activation='relu')) # Added a dense layer\n",
        "model4.add(Dense(units=2, activation='softmax'))  # Sigmoid for binary classification\n",
        "\n",
        "# Compile model4\n",
        "model4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model4 with GPU (or CPU)\n",
        "with tf.device('/GPU:0'):  # Or '/CPU:0'\n",
        "    model4.fit(Xtrain, Ytrain, epochs=15, batch_size=64, validation_data=(Xtest, Ytest))\n",
        "\n",
        "\n",
        "# Evaluate model4\n",
        "loss, accuracy = model4.evaluate(Xtest, Ytest)\n",
        "print(f\"Model 4 - Test Loss: {loss}\")\n",
        "print(f\"Model 4 - Test Accuracy: {accuracy}\")\n",
        "\n",
        "plot_model(model4)\n",
        "model4.save('model4.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XZCCMYfYasAX",
        "outputId": "1bbb519d-7521-4fc4-ab6f-ae6c7eaa552a"
      },
      "outputs": [],
      "source": [
        "plot_model(model4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7wGA9A1bOCn",
        "outputId": "3755aa79-1fd6-4e58-87cc-b34c2f54eb5b"
      },
      "outputs": [],
      "source": [
        "model4.save('forth_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "gnIb9HmzU5gG",
        "outputId": "5cd40d4d-333f-48e2-ea29-25a5911f0638"
      },
      "outputs": [],
      "source": [
        "# Faux , faux\n",
        "# vrai faux\n",
        "# vrai , vrai\n",
        "# faux , vrai\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    data = pd.read_excel(\"balanced_dataset.xlsx\")\n",
        "\n",
        "    # Create new columns for the four classes\n",
        "    data['class_1'] = 0  # False, False\n",
        "    data['class_2'] = 0  # True, False\n",
        "    data['class_3'] = 0  # True, True\n",
        "    data['class_4'] = 0  # False, True\n",
        "\n",
        "    # Assign values to the new classes based on 'Robot_ProtectiveStop' and 'grip_lost'\n",
        "    data.loc[(data['Robot_ProtectiveStop'] == False) & (data['grip_lost'] == False), 'class_1'] = 1\n",
        "    data.loc[(data['Robot_ProtectiveStop'] == True) & (data['grip_lost'] == False), 'class_2'] = 1\n",
        "    data.loc[(data['Robot_ProtectiveStop'] == True) & (data['grip_lost'] == True), 'class_3'] = 1\n",
        "    data.loc[(data['Robot_ProtectiveStop'] == False) & (data['grip_lost'] == True), 'class_4'] = 1\n",
        "\n",
        "    # Drop the original combined target columns\n",
        "    data = data.drop(['Robot_ProtectiveStop', 'grip_lost'], axis=1)\n",
        "\n",
        "    # Save the modified DataFrame to a new Excel file\n",
        "    final_db_file = \"final_dataset_four_classes.xlsx\"\n",
        "    data.to_excel(final_db_file, index=False)\n",
        "    print(f\"Final database with four classes saved to {final_db_file}\")\n",
        "\n",
        "    from google.colab import files\n",
        "    files.download(final_db_file)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"'balanced_dataset.xlsx' not found. Please make sure the file exists in the current directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52zaq2HfXjcH",
        "outputId": "1eb78d68-ea50-4976-d123-8e875cd91ac5"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Define your target variable (y) as one of the four classes, for example 'class_1'\n",
        "y = data['class_1']  # Replace with the desired class\n",
        "\n",
        "# Define your features (X) as all columns except the target class columns\n",
        "X = data.drop(['class_1', 'class_2', 'class_3', 'class_4'], axis=1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train a Decision Tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)  # You can tune hyperparameters here\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AA3SSllYvRA",
        "outputId": "04c6caf5-bb3d-4355-e540-4d596b4177c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming y_test and y_pred are already defined from your model's predictions\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='weighted') #or 'macro', 'micro'\n",
        "print(f\"Precision: {precision}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='weighted') #or 'macro', 'micro'\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# F1-Score\n",
        "f1 = f1_score(y_test, y_pred, average='weighted') #or 'macro', 'micro'\n",
        "print(f\"F1-Score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "y6fqwJcLckLj",
        "outputId": "6af3aba4-ef86-4c81-f4e9-b6d282f6b9de"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Save the trained Decision Tree model to a file\n",
        "dump(clf, 'decision_tree_model.joblib')\n",
        "print(\"Decision Tree model saved to decision_tree_model.joblib\")\n",
        "\n",
        "# Download the saved model file\n",
        "files.download('decision_tree_model.joblib')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIGpYv8OY4n1",
        "outputId": "b7d11517-8c06-4eea-b38b-7ac351b86a24"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined from previous code\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model (example using accuracy)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf}\")\n",
        "\n",
        "# You can also calculate other metrics like precision, recall, F1-score, etc. as shown in the previous example\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cYw_yQcpc1ci",
        "outputId": "3ee49836-5b68-48c9-85ad-ab455883f44f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Save the trained Random Forest model to a file\n",
        "dump(rf_classifier, 'random_forest_model.joblib')\n",
        "print(\"Random Forest model saved to random_forest_model.joblib\")\n",
        "\n",
        "# Download the saved model file\n",
        "files.download('random_forest_model.joblib')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACpvyWXUY-Y3",
        "outputId": "ffc9e19b-82f6-4da3-fd71-b213104a9540"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],  # Splitting criterion\n",
        "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4]  # Minimum samples required to be at a leaf node\n",
        "}\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy') # 5-fold cross-validation\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {best_score}\")\n",
        "\n",
        "# Train a new Decision Tree model with the best hyperparameters\n",
        "best_clf = DecisionTreeClassifier(**best_params, random_state=42)\n",
        "best_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model with the best hyperparameters\n",
        "y_pred_best = best_clf.predict(X_test)\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "print(f\"Accuracy with Best Hyperparameters: {accuracy_best}\")\n",
        "\n",
        "# Print classification report for the best model\n",
        "print(classification_report(y_test, y_pred_best))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "zIlaO82obge6",
        "outputId": "d49ffec1-7227-4726-c2b9-ed3aa5b8a150"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clATbzHMcdEO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
